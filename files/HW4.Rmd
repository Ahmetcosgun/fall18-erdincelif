---
title: "IE 582 - Homework - 4"
author: "Elif Erdinç"
date: "December 19, 2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

In this project, penalized regression, decision trees, support vector machines, random forest, and stochastic gradient boosting methods are applied on two datasets (regression and classification). First train and test datasets are created from original data randomly in a ratio of 7:3, respectively.

Caret package is used to estimate the best parameters for each method using 10-fold cross-validation. Models are created from training data. The quality of the estimations are calculated based on the predictions on both training and test data. 

For regression data, since output is numeric, the metric for the quality of the estimation is selected as root mean square error (RMSE). Lower the RMSE, better the fit of the model. On the other hand, for classification data, confusion matrices are formed and accuracies are calculated.


## 2. Regression Data

This dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson's disease. Data contain subject number, subject age, subject gender, time interval from baseline recruitment date, motor UPDRS, total UPDRS, and 16 biomedical voice measures. There are 5875 instances and 21 attributes with one categorical (gender) and remainings with numeric. 

The link of the dataset is: http://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring 

RMSE results of train and test RMSE evaluations are performed. In order to make a reasonable comparison to see how good the RMSE is, the values of outputs are also significant. Average of output values for train and test data are 21.29266 and 21.30455.   

### 2.1. Penalized Regression Approaches (PRA)

As a penalized regression method, Lasso regression is chosen and for regression. Optimal lambda value is tried to be found for the best model based on the training data. Using the "train" function of caret package and setting the method as "rqlasso", the optimal lambda value is calculated as 1e-04. 

To compare prediction adequacy, RMSE model (train) and test are estimated as 0.586157 and 0.6354336, respectively. Compared to output range, RMSE values are quite low which means that model is very successful. The reason behind the good model comes from the fact that all attributes accept 1 categorical one are all numeric.  

### 2.2. Decision Trees (DT)

In decision tree model, complexity parameter value is tried to be optimized to construct the best model based on the training data. Using the "train" function of caret package and setting the method as "rpart", the optimal Cp value is calculated as 0.03673525.

To compare prediction adequacy, RMSE model (train) and test are estimated as 7.261615e-14 and 0.05733861, respectively. Compared to output range, RMSE values are quite low which means that model is very successful.Also, model RMSE lower than test RMSE makes sense showing that overfitting is not an issue in the model. 


### 2.3. Support Vector Machines (SVM)

In SVM, two parameters, penalty parameter (referred to as C) and kernel type, are aimed to be optimized. Both radial (Gaussian) and polynomial kernel types are studied.

#### 2.3.1. Support Vector Machines (SVM) - Radial

In radial kernel model, using the "train" function of caret package and setting the method as "svmRadial", the optimal bandwidth value (sigma) is calculated as 0.08403207.

RMSE model (train) and test are estimated as 0.02712914 and 0.106729, respectively. Compared to output range, RMSE values are quite low which means that model is very successful.


#### 2.3.2. Support Vector Machines (SVM) - Polynomial

In polynomial kernel model, using the "train" function of caret package and setting the method as "svmPoly", the optimal degree is found as 2 where scale is 0.1 and C is 0.5.

RMSE model (train) and test are estimated as 0.3798106 and 0.6028663, respectively. Compared to output range, RMSE values are quite low which means that model is very successful.

In both SVM kernel cases, model RMSE lower than test RMSE makes sense showing that overfitting is not an issue in the model. When radial and polynomial kernels are compared, radial (Gaussian) one gives better results than polynomial one.

### 2.4. Random Forest (RF)

In random forest model, the minimal number of observations per tree leaf is tried to be optimized to construct the best model based on the training data. Using the "train" function of caret package and setting the method as "rf", the optimal mtry value is calculated as 19. Compared to classification data and regarding the number of attributes, it looks large.

RMSE model (train) and test are estimated as 0.004256357 and 0.027517, respectively. Compared to output range, RMSE values are quite low which means that model is very successful.

### 2.5. Stochastic Gradient Boosting (SGB)

In SGB model, the depth, learning rate, number of trees, and minimal number of observations per tree leaf are aimed to be optimized to construct the best model based on the training data. Using the "train" function of caret package and setting the method as "xgbTree", the optimal values for max_depth, eta, and colsample_bytree are found as 3, 0.3, and 0.8, respectively. 


RMSE model (train) and test are estimated as 0.0002760811 and 0.01367807, respectively. Compared to output range, RMSE values are quite low which means that model is very successful.

## 3. Classification Data

The data set consists of the expression levels of 77 proteins/protein modifications that produced detectable signals in the nuclear fraction of cortex. There are 38 control mice and 34 trisomic mice (Down syndrome), for a total of 72 mice. In the experiments, 15 measurements were registered of each protein per sample/mouse. Therefore, for control mice, there are 38x15, or 570 measurements, and for trisomic mice, there are 34x15, or 510 measurements. The dataset contains a total of 1080 measurements per protein. Each measurement can be considered as an independent sample/mouse. 

The eight classes of mice are described based on features such as genotype, behavior and treatment. According to genotype, mice can be control or trisomic. According to behavior, some mice have been stimulated to learn (context-shock) and others have not (shock-context) and in order to assess the effect of the drug memantine in recovering the ability to learn in trisomic mice, some mice have been injected with the drug and others have not. 

The link of the dataset is: http://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression

Although this dataset includes 82 attributes, there are too many missing values. So, I first tried remove the rows including NA values. However, in this case since the number of instances are reduced too much, ratio of # of instances/# of variables decreased in a way that models fit perfectly well. Therefore, I removed the columns with NA values. The final dataset has 28 attributes with 1080 observations. There are 8 classes in string form, and all attributes are numeric. 


### 3.1. Penalized Regression Approaches (PRA)

In Lasso regression, lambda value is tried to be optimized to construct the best model based on the training data. Using the "train" function of caret package and setting the method as "pda", the optimal lambda value is calculated as 1e-04.

The model is used to predict the output in both training and test data. Confusion matrices for training and test data are given in **Tables 1 and 2**. 

![**Table 1** - Regression data - Penalized Regression - confusion matrix for train data.](C:/Users/09172/Desktop/hw4pic/Capture1.png)


![**Table 2** - Regression data - Penalized Regression - confusion matrix for test data.](C:/Users/09172/Desktop/hw4pic/Capture.png)


The accuracies of the models for training and test are 0.8505291 and 0.8179012, respectively. Model accuracy is found slightly larger than test accuracy. This shows that model reasonably fits well. Moreover, test and train data are separated randomly. These accuracies and confusion matrices shows uniform distribution among classes.  


### 3.2. Decision Trees (DT)

In decision tree model, complexity parameter value is tried to be optimized to construct the best model based on the training data. Using the "train" function of caret package and setting the method as "rpart", the optimal Cp value is calculated as 0.06702619.

The model is used to predict the output in both training and test data. Confusion matrices for training and test data are given in **Tables 3 and 4**. 
  

![**Table 3** - Regression data - Decision tree - confusion matrix for train data.](C:/Users/09172/Desktop/hw4pic/rdt2.png)

![**Table 4** - Regression data - Decision tree - confusion matrix for test data.](C:/Users/09172/Desktop/hw4pic/rdt1.png)

The accuracies of the models for training and test are 0.3320106 and 0.3240741, respectively. Both model and test accuracies are found quite low compared to other classification methods. This shows that model is not adequate. The reason behind this model inadequecy may stem from the fact that the numeric attributes does not work well in decision trees.


### 3.3. Support Vector Machines (SVM)
In SVM, two parameters, penalty parameter (referred to as C) and kernel type, are aimed to be optimized. Both radial (Gaussian) and polynomial kernel types are studied.

#### 3.3.1.  Support Vector Machines (SVM) - Radial

In radial kernel model, using the "train" function of caret package and setting the method as "svmRadial", the optimal bandwidth value (sigma) is calculated as 0.0254633.

The model is used to predict the output in both training and test data. Confusion matrices for training and test data are given in **Tables 5 and 6**.


![**Table 5** - Regression data - SVM (radial) - confusion matrix for train data.](C:/Users/09172/Desktop/hw4pic/Capture4.png)



![**Table 6** - Regression data - SVM (radial) - confusion matrix for test data.](C:/Users/09172/Desktop/hw4pic/Capture3.png)

The accuracies of the models for training and test are 0.9973545 and 0.9845679, respectively. Both accuracies are quite high, and model accuracy is found slightly larger than test accuracy. This shows that model reasonably fits well. 


#### 3.3.2.  Support Vector Machines (SVM) - Polynomial

In polynomial kernel model, using the "train" function of caret package and setting the method as "svmPoly", the optimal degree is found as 3 where scale is 0.1 and C is 0.5.

The model is used to predict the output in both training and test data. Confusion matrices for training and test data are given in **Tables 7 and 8**.


![**Table 7** - Regression data - SVM (polynomial) - confusion matrix for train data.](C:/Users/09172/Desktop/hw4pic/Capture6.png)

![**Table 8** - Regression data - SVM (polynomial) - confusion matrix for test data.](C:/Users/09172/Desktop/hw4pic/Capture5.png)


The accuracies of the models for training and test are 1 and 0.9938272, respectively. Model accuracy of 1 and test accuracy higher than 0.99 shows good fit of model.


### 3.4. Random Forest (RF)

In random forest model, the minimal number of observations per tree leaf is tried to be optimized to construct the best model based on the training data. Using the "train" function of caret package and setting the method as "rf", the optimal mtry value is calculated as 2.

The model is used to predict the output in both training and test data. Confusion matrices for training and test data are given in **Tables 9 and 10**. 


![**Table 9** - Regression data - Random forest - confusion matrix for train data.](C:/Users/09172/Desktop/hw4pic/Capture8.png)

![**Table 10** - Regression data - Random forest - confusion matrix for test data.](C:/Users/09172/Desktop/hw4pic/Capture7.png)


The accuracies of the models for training and test are 1 and 0.9845679, respectively. Model accuracy is found as 1 meaning that model predicts the class without an error. This might be seen as overfitting in the model, however since test accuracy is also quite high, this shows that model reasonably fits well. 


### 3.5. Stochastic Gradient Boosting (SGB)

In SGB model, the depth, learning rate, number of trees, and minimal number of observations per tree leaf are aimed to be optimized to construct the best model based on the training data. Using the "train" function of caret package and setting the method as "xgbTree", the optimal values for max_depth, eta, and colsample_bytree are found as 3, 0.3, and 0.8, respectively. 

The model is used to predict the output in both training and test data. Confusion matrices for training and test data are given in **Tables 11 and 12**. 

![**Table 11** - Regression data - SGB - confusion matrix for train data.](C:/Users/09172/Desktop/hw4pic/Capture10.png)


![**Table 12** - Regression data - SGB - confusion matrix for test data.](C:/Users/09172/Desktop/hw4pic/Capture9.png)



The accuracies of the models for training and test are 1 and 0.9722222, respectively. Model accuracy is found as 1 meaning that model predicts the class without an error. This might be seen as overfitting in the model, however since test accuracy is also quite high, this shows that model reasonably fits well. 

# 4. Discussion

In both cases, the prediction ability of SGB model is found the best. The worst model and test predictions are seen in decision tree method on the regression data due to the mostly numeric atrributes. Also, especially in the case of classification, data fit well in all models. Both high model and test accuracies means models fit well, there could be overfitting if the variable and output domains are not changing in a large extent.

# 5. Appendix  

## 5.1. Code for Regression Data

```{r,include=TRUE,eval=FALSE}
#include=TRUE writes the code on document
#eval=FALSE doesn't run the code during knitting
require(penalized)
library(ISLR)
library(glmnet)
library(dplyr)
library(tidyr)

data=read.csv(file="C:/Users/09172/Desktop/parkinsons_updrs.csv", header = TRUE, sep = ",")
data= as.data.frame(data)

data[,3] = as.factor(data[,3])

# data=data[data$V1>=2005,]

smp_size = floor(0.70*nrow(data))

set.seed(123)
train_ind=sample(seq_len(nrow(data)), size = smp_size)

train= data[train_ind,]
test= data[-train_ind,]

trainclass=train[,21]
traindata=train[,2:20]
testclass=test[,21]
testdata=test[,2:20]


#install.packages('caret', dependencies=T)
library(caret)

d =cbind(trainclass,traindata)

# LASSO

# fit1 = optL1(trainclass, traindata, lambda2=0, fold=10)
# pen=penalized(trainclass~., data = traindata, lambda1=0.3911655,
#             lambda2=0, model = "linear", standardize = FALSE, trace = TRUE)
# # 
# train_pred = predict(pen, traindata)
# MSE_train= mean((as.data.frame(train_pred)$mu - trainclass)^2)
# 
# test_pred= predict(pen, testdata)
# MSE_test = mean((as.data.frame(test_pred)$mu - testclass)^2)


#fit <- profL1(trainclass, traindata, fold=10, plot=TRUE)
# 
# list=seq(0, 2, by = 0.5)
# for (i in list){
# pen=penalized(trainclass~., data = traindata, lambda1=i,
#               lambda2=0, model = "poisson", standardize = FALSE, trace = TRUE)
# y[i] = sum(pen@residuals^2)/nrow(train)
# }
# plot(list,y)
# list
# y
# 
# # ###
# grid = 10^seq(10, -2, length = 100)
# lasso_mod = glmnet(as.matrix(traindata), trainclass, alpha = 1, lambda = grid) # Fit lasso model on training data
# 
# plot(lasso_mod)
# 
# cv.out = cv.glmnet(as.matrix(traindata), trainclass, alpha = 1) # Fit lasso model on training data
# plot(cv.out) # Draw plot of training MSE as a function of lambda
# bestlam = cv.out$lambda.min # Select lamda that minimizes training MSE
# train_MSE = min(cv.out$cvm)
# 
# lasso_pred = predict(lasso_mod, s = bestlam, newx = as.matrix(testdata)) # Use best lambda to predict test data
# lasso_pred = round(lasso_pred)
# test_MSE = mean((lasso_pred - testclass)^2) # Calculate test MSE

model_pen <- train(trainclass~., data =d,  method = "rqlasso",
                  trControl = trainControl("cv", number = 10))

model_pen$bestTune
predictions_las_test = predict(model_pen,testdata)
MSE_test_las = mean(predictions_las_test - testclass)^2


predictions_las_train = predict(model_pen,traindata)
MSE_train_las = mean(predictions_las_train - trainclass)^2


# SVM

# install.packages("e1071")
# library("e1071")

# tuned_parameters <- tune.svm(as.matrix(trainclass)~as.matrix(traindata), gamma = 10^(-5:-1), cost = 10^(-3:1))
# 
# summary(tuned_parameters)
# svm_model = svm(as.matrix(trainclass)~as.matrix(traindata),  kernel = "polynomial", gamma = 1e-05, cost = 0.001)
# 
# my_prediction <- predict(svm_model, testdata)
# MSE_test_svm = mean((as.data.frame(test_pred)$mu - testclass)^2)


# svm_tune <- tune(svm, train.x=traindata, train.y=trainclass, 
#                  kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
# 
# print(svm_tune)
# 
# svm_model_after_tune <- svm(as.matrix(trainclass)~as.matrix(traindata),  kernel="radial", cost=10, gamma=0.5)
# summary(svm_model_after_tune)
# my_prediction <- predict(svm_model_after_tune, testdata)
# MSE_train_svm = mean((my_prediction - trainclass)^2)
# 
# svm_model_test <- svm(as.matrix(testclass)~as.matrix(testdata),  kernel="radial", cost=10, gamma=0.5)
# summary(svm_model_test)
# my_prediction_test <- predict(svm_model_test, testdata)
# MSE_test_svm = mean((my_prediction_test - testclass)^2)
# 
# 
# svm_tune_2 <- tune(svm, train.x=traindata, train.y=trainclass, 
#                  kernel="polynomial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))


# Radial 
model_svmR <- train(trainclass~., data =d,  method = "svmRadial",
                   trControl = trainControl("cv", number = 10))

model_svmR$bestTune
predictions_svmR_test = predict(model_svmR,testdata)
MSE_test_svmR = mean(predictions_svmR_test - testclass)^2

predictions_svmR_train = predict(model_svmR,traindata)
MSE_train_svmR = mean(predictions_svmR_train - trainclass)^2

# Polynomial
model_svmP <- train(trainclass~., data =d,  method = "svmPoly",
                    trControl = trainControl("cv", number = 10))

model_svmP$bestTune
predictions_svmP_test = predict(model_svmP,testdata)
MSE_test_svmP = mean(predictions_svmP_test - testclass)^2

predictions_svmP_train = predict(model_svmP,traindata)
MSE_train_svmP = mean(predictions_svmP_train - trainclass)^2

# DECISION TREE

# library(rpart)
# 
# fit <- rpart(as.matrix(trainclass)~as.matrix(traindata), 
#              method="anova", data=cu.summary, control)
# 
# dt_tune= tune.rpart(as.matrix(trainclass)~as.matrix(traindata), data= traindata, minbucket = c(5,10,15,20),
#            cp = c(0.005, 0.01,0.02,0.04, 0.05, 0.1))
# plot(dt_tune)
# 
# dt_model = rpart(as.matrix(trainclass)~as.matrix(traindata), data= traindata, control = rpart.control(cp = 0.005, minbucket = 5))
# 
# summary(dt_model)
# 
# predictions_dt = predict(dt_model, traindata)
# MSE_train_dt = mean((predictions_dt - trainclass)^2)
# 
# dt_model_test = rpart(as.matrix(testclass)~as.matrix(testdata), data= testdata, control = rpart.control(cp = 0.005, minbucket = 5))
# MSE_test_dt = mean((predict(dt_model_test, testdata) - testclass)^2)


model_dt <- train(trainclass~., data =d,  method = "rpart",
                    trControl = trainControl("cv", number = 10))

model_dt$bestTune
predictions_dt_test = predict(model_dt,testdata)
MSE_test_dt = mean(predictions_dt_test - testclass)^2

predictions_dt_train = predict(model_dt,traindata)
MSE_train_dt = mean(predictions_dt_train - trainclass)^2
# 
# printcp(fit) # display the results 
# plot(fit) # visualize cross-validation results 
# summary(fit) # detailed summary of splits

#RANDOM FOREST

d =cbind(trainclass,traindata)
model_rf <- train(trainclass~., data =d,  method = "rf",
               trControl = trainControl("cv", number = 10))

model_rf$bestTune
predictions_rf_test = predict(model_rf,testdata)
MSE_test_rf = mean(predictions_rf_test - testclass)^2

predictions_rf_train = predict(model_rf,traindata)
MSE_train_rf = mean(predictions_rf_train - trainclass)^2


# XGBOOST
# install.packages("caret", repos=c("http://rstudio.org/_packages", "http://cran.rstudio.com",dependencies=TRUE))
# require(caret)


model <- train(trainclass~., data =d,  method = "xgbTree",
  trControl = trainControl("cv", number = 10)
)
model$bestTune
predictions_xg = predict(model,testdata)
MSE_test_xg = mean(predictions_xg - testclass)^2

predictions_xg_train = predict(model,traindata)
MSE_train_xg = mean(predictions_xg_train - trainclass)^2

```


## 5.2. Code for Classification Data

```{r,include=TRUE,eval=FALSE}
#include=TRUE writes the code on document
#eval=FALSE doesn't run the code during knitting
data=read.csv(file="C:/Users/09172/Desktop/HW4/mice.csv", header = TRUE, sep = ",")
data= as.data.frame(data)
#data =data[complete.cases(data), ]
data[,1]=NULL
data=data[,colSums(is.na(data)) == 0] 
data[,c(29,30,31)]=NULL

# data[,32] = as.factor(data[,32])
# data[,31] = as.factor(data[,31])
# data[,30] = as.factor(data[,30])
data[,29] = as.factor(data[,29])

smp_size = floor(0.70*nrow(data))

set.seed(123)
train_ind=sample(seq_len(nrow(data)), size = smp_size)

train= data[train_ind,]
test= data[-train_ind,]

trainclass=train[,29]
traindata=train[,1:28]
testclass=test[,29]
testdata=test[,1:28]

#install.packages('caret', dependencies=T)
require(caret)

d =cbind(trainclass,traindata)

# LASSO

require(mda)
model_pen <- train(trainclass~., data =d,  method = "pda", metric = "Accuracy", maximize = TRUE,  
                   trControl = trainControl("cv", number = 10))

model_pen$bestTune
predictions_las_test = predict(model_pen,testdata)
#MSE_test_las = mean(predictions_las_test - testclass)^2

tbl_las_1=table(predictions_las_test, testclass)
acc_las_test=sum(diag(tbl_las_1))/length(testclass)

predictions_las_train = predict(model_pen,traindata)
#MSE_train_las = mean(predictions_las_train - trainclass)^2

tbl_las_2=table(predictions_las_train, trainclass)
acc_las_train=sum(diag(tbl_las_2))/length(trainclass)


# SVM

# Radial 
model_svmR <- train(trainclass~., data =d,  method = "svmRadial",
                    trControl = trainControl("cv", number = 10))

model_svmR$bestTune
predictions_svmR_test = predict(model_svmR,testdata)


tbl_svmR_1=table(predictions_svmR_test, testclass)
acc_svmR_test=sum(diag(tbl_svmR_1))/length(testclass)
#MSE_test_svmR = mean(predictions_svmR_test - testclass)^2

predictions_svmR_train = predict(model_svmR,traindata)
#MSE_train_svmR = mean(predictions_svmR_train - trainclass)^2
tbl_svmR_2=table(predictions_svmR_train, trainclass)
acc_svmR_train=sum(diag(tbl_svmR_2))/length(trainclass)

# Polynomial
model_svmP <- train(trainclass~., data =d,  method = "svmPoly",
                    trControl = trainControl("cv", number = 10))

model_svmP$bestTune
predictions_svmP_test = predict(model_svmP,testdata)

tbl_svmP_1=table(predictions_svmP_test, testclass)
acc_svmP_test=sum(diag(tbl_svmP_1))/length(testclass)

#MSE_test_svmP = mean(predictions_svmP_test - testclass)^2

predictions_svmP_train = predict(model_svmP,traindata)
#MSE_train_svmP = mean(predictions_svmP_train - trainclass)^2
tbl_svmP_2=table(predictions_svmP_train, trainclass)
acc_svmP_train=sum(diag(tbl_svmP_2))/length(trainclass)

# DECISION TREE

model_dt <- train(trainclass~., data =d,  method = "rpart",
                  trControl = trainControl("cv", number = 10))

model_dt$bestTune
predictions_dt_test = predict(model_dt,testdata)
#MSE_test_dt = mean(predictions_dt_test - testclass)^2
tbl_dt_1=table(predictions_dt_test, testclass)
acc_dt_test=sum(diag(tbl_dt_1))/length(testclass)

predictions_dt_train = predict(model_dt,traindata)
#MSE_train_dt = mean(predictions_dt_train - trainclass)^2
tbl_dt_2=table(predictions_dt_train, trainclass)
acc_dt_train=sum(diag(tbl_dt_2))/length(trainclass)

#RANDOM FOREST

# d =cbind(trainclass,traindata)
model_rf <- train(trainclass~., data =d,  method = "rf",
                  trControl = trainControl("cv", number = 10))

model_rf$bestTune
predictions_rf_test = predict(model_rf,testdata)
#MSE_test_rf = mean(predictions_rf_test - testclass)^2
tbl_rf_1=table(predictions_rf_test, testclass)
acc_rf_test=sum(diag(tbl_rf_1))/length(testclass)

predictions_rf_train = predict(model_rf,traindata)
#MSE_train_rf = mean(predictions_rf_train - trainclass)^2
tbl_rf_2=table(predictions_rf_train, trainclass)
acc_rf_train=sum(diag(tbl_rf_2))/length(trainclass)

# XGBOOST

model_xg <- train(trainclass~., data =d,  method = "xgbTree",
               trControl = trainControl("cv", number = 10)
)
model_xg$bestTune
predictions_xg_test = predict(model_xg,testdata)
#MSE_test_xg = mean(predictions_xg - testclass)^2
tbl_xg_1=table(predictions_xg_test, testclass)
acc_xg_test=sum(diag(tbl_xg_1))/length(testclass)

predictions_xg_train = predict(model_xg,traindata)
#MSE_train_xg = mean(predictions_xg_train - trainclass)^2
tbl_xg_2=table(predictions_xg_train, trainclass)
acc_xg_train=sum(diag(tbl_xg_2))/length(trainclass)
```


